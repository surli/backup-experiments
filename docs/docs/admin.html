
<!DOCTYPE html>
<!--
 Generated by Apache Maven Doxia at 2016-10-03
 Rendered using Reflow Maven Skin 1.1.1 (http://andriusvelykis.github.io/reflow-maven-skin)
-->
<html  xml:lang="en" lang="en">

	<head>
		<meta charset="UTF-8" />
		<title>Oryx &#x2013; Docs: Admin</title>
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<meta name="description" content="" />
		<meta http-equiv="content-language" content="en" />

		<link href="http://netdna.bootstrapcdn.com/bootswatch/2.3.2/united/bootstrap.min.css" rel="stylesheet" />
		<link href="http://netdna.bootstrapcdn.com/twitter-bootstrap/2.3.1/css/bootstrap-responsive.min.css" rel="stylesheet" />
		<link href="../css/bootswatch.css" rel="stylesheet" />
		<link href="../css/reflow-skin.css" rel="stylesheet" />


		<link href="../css/lightbox.css" rel="stylesheet" />

		<link href="../css/site.css" rel="stylesheet" />
		<link href="../css/print.css" rel="stylesheet" media="print" />

		<!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
		<!--[if lt IE 9]>
			<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
		<![endif]-->



	</head>

	<body class="page-docs-admin project-oryx" data-spy="scroll" data-offset="60" data-target="#toc-scroll-target">

		<div class="navbar navbar-fixed-top">
			<div class="navbar-inner">
				<div class="container">
					<a class="btn btn-navbar" data-toggle="collapse" data-target="#top-nav-collapse">
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
					</a>
					<a class="brand" href="index.html">Oryx 2</a>
					<div class="nav-collapse collapse" id="top-nav-collapse">
						<ul class="nav pull-right">
							<li ><a href="../index.html" title="Overview">Overview</a></li>
							<li ><a href="endusers.html" title="Docs: End Users">Docs: End Users</a></li>
							<li ><a href="developer.html" title="Docs: Dev">Docs: Dev</a></li>
							<li class="active"><a href="" title="Docs: Admin">Docs: Admin</a></li>
							<li ><a href="performance.html" title="Performance">Performance</a></li>
							<li ><a href="../apidocs/index.html" title="Javadoc">Javadoc</a></li>
							<li ><a href="https://github.com/OryxProject/oryx/releases" title="Download" class="externalLink">Download</a></li>
						</ul>
					</div><!--/.nav-collapse -->
				</div>
			</div>
		</div>

	<div class="container">

	<!-- Masthead
	================================================== -->

	<header>
	</header>

	<div class="main-body">
	<div class="row">
		<div class="span8">
			<div class="body-content">
<div class="page-header">
 <h1 id="cluster_setup">Cluster Setup</h1>
</div> 
<p>The following are required as of Oryx 2.2.0:</p> 
<ul> 
 <li>Java 8 or later (JRE only is required)</li> 
 <li>A Hadoop cluster running the following components: 
  <ul> 
   <li>Apache Hadoop 2.6.0 or later</li> 
   <li>Apache Zookeeper 3.4.5 or later</li> 
   <li>Apache Kafka 0.9 or later</li> 
   <li>Apache Spark 1.6.0 or later</li> 
  </ul></li> 
</ul> 
<p><a class="externalLink" href="http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html">CDH</a> 5.7.0 and later meet these requirements, although any Hadoop distribution with these components should work fine. While the rest of the instructions will refer to a CDH 5.7+ distribution, this is not a requirement.</p> 
<p><i>Note: Oryx 2.0.x requires only Java 7, Spark 1.3.0, Kafka 0.8 and CDH 5.4.x</i></p> 
<p><i>Note: Oryx 2.1.x requires only Java 7, Spark 1.5.0, Kafka 0.8 and CDH 5.5.x</i></p> 
<p>A single-node cluster can be sufficient, although running all of these components on one machine may require a reasonable amount of RAM.</p> 
<div class="section"> 
 <h2 id="Deployment_Architecture">Deployment Architecture</h2> 
 <p>Because the Batch and Speed Layers are Spark applications, they need to run within a cluster. The applications themselves run the driver for these Spark applications, and these may run on an edge node in a cluster like any other Spark application.. That is, the binaries themselves do not need to run on a node that also runs a particular service, but, it will need to run on a node within the cluster because both Layer application interact extensively with compute and storage within the cluster.</p> 
 <p>The Serving Layer may be run within the cluster too, and may be run via YARN on any node. However it’s common to consider deploying this Layer, which exposes an API to external services, on a node that is not within the cluster. This is possible. The Serving Layer must be able to communicate with a Kafka broker, at a minimum.</p> 
 <p>In some applications, the Serving Layer also needs to read large models directly from HDFS. In these cases, it would also have to access HDFS. This is only required in applications that must write large models to HDFS. This is closely related to <tt>oryx.update-topic.message.max-size</tt> and the maximum size message that Kafka can support.</p> 
</div> 
<div class="section"> 
 <h2 id="Services">Services</h2> 
 <p>Install and configure the Hadoop cluster normally. The following services need to be enabled:</p> 
 <ul> 
  <li>HDFS</li> 
  <li>YARN</li> 
  <li>Zookeeper</li> 
  <li>Kafka</li> 
  <li>Spark</li> 
 </ul> 
 <p>Note that for CDH, Kafka is available as a parcel from <a class="externalLink" href="http://www.cloudera.com/content/cloudera/en/developers/home/cloudera-labs/apache-kafka.html">Cloudera Labs</a>. The Cloudera Kafka 2.x parcel is required, because it contains a distribution of Kafka 0.9. The 2.x parcel is in fact required by CDH 5.7.</p> 
 <p>Determine the (possibly several) Kafka brokers that are configured in the cluster, under Instances, and note their hosts and port. The port is typically 9092. Same for the Zookeeper servers; the default port here is 2181. Default ports will be used in subsequent examples.</p> 
 <p>Where a Kafka broker or Zookeeper server is called for, you can and should specify a comma-separated list of <tt>host:port</tt> pairs where there are multiple hosts. Example: <tt>your-zk-1:2181,your-zk-2:2181</tt>.</p> 
</div> 
<div class="section"> 
 <h2 id="Java">Java</h2> 
 <p>Java 8 (JRE) needs to be installed on all nodes on the cluster. Cluster processes need to use Java 8. Depending on the nature of your Hadoop cluster installation, this may mean updating the default Java version with <tt>update-alternatives --config java</tt> or equivalent, or setting <tt>JAVA_HOME</tt> to point to a Java 8 installation.</p> 
 <p>For CDH in particular, however, instead see <a class="externalLink" href="http://www.cloudera.com/documentation/enterprise/latest/topics/cm_ig_java_home_location.html">Configuring a Custom Java Home Location</a></p> 
</div> 
<div class="section"> 
 <h2 id="Configuring_Kafka">Configuring Kafka</h2> 
 <p>Oryx will use two Kafka topics for data transport. One carries input data to the batch and Speed Layer, and the other carries model updates from there on to the Serving Layer. The default names of these topics are <tt>OryxInput</tt> and <tt>OryxUpdate</tt> respectively. They need to be created before Oryx is started.</p> 
 <p>The number of partitions for the <i>input</i> topic will affect the number of partitions, and therefore parallelism, of the Spark Streaming jobs that consume them. For example, the Batch Layer reads partitions of historical data from HDFS and from Kafka. If the input topic has just one partition but a large amount of data arrives per interval, then the Kafka-based partition of the input may be relatively very large and take a long time to process. A good rule of thumb may be to choose a number of topic partitions such that the amount of data that arrives in one batch interval, per partition, is expected to be under the size of one HDFS block, which is 128MB by default. So if you have 1.28GB arriving per batch interval, at least 10 partitions is probably a good idea to make sure the data can be processed in reasonably sized chunks, and with enough parallelism.</p> 
 <p>The provided <tt>oryx-run.sh kafka-setup</tt> script configures a default of 4 partitions, but this can be changed later. Note that there is no purpose in configuring more than 1 partition for the <i>update</i> topic.</p> 
 <p>Replication factor can be any value, but at least 2 is recommended. Note that the replication factor can’t exceed the number of Kafka brokers in the cluster. The provided script sets replication to 1, by default, for this reason. This can be changed later with, for example, <tt>kafka-topics --zookeeper ... --alter --topic ... --replication-factor N</tt></p> 
 <p>You may need to configure the retention time for one or both topics. In particular, it’s typically important to limit the retention time for the update topic, since the Speed and Serving Layer read the entire topic from the start on startup to catch up. This is not as important for the input topic, which is not re-read from the beginning.</p> 
 <p>Setting it to twice the Batch Layer update interval is a good start. For example, to set it to 1 day (24 * 60 * 60 * 1000 = 86400000 ms), set the topic’s <tt>retention.ms</tt> property to 86400000. This is done automatically by the provided <tt>oryx-run.sh kafka-setup</tt> script.</p> 
 <p>The two topics above may contain large messages; in particular the update topic includes entire serialized PMML models. It’s possible that they exceed Kafka’s default max message size of 1 MiB. If large models are expected, then the topic’s <tt>max.message.bytes</tt> should be configured to allow larger messages. <tt>oryx-run.sh kafka-setup</tt> sets a default of 16 MiB for the update topic. This is also the default maximum size of a model that Oryx will attempt to write to the update topic; larger models will be passed as a reference to the model file’s location on HDFS instead. See setting <tt>oryx.update-topic.message.max-size</tt>.</p> 
 <p>The Kafka broker’s <tt>message.max.bytes</tt> (note the different name!) property also controls this, but setting it affects all topics managed by the broker, which may be undesirable. See <a class="externalLink" href="http://www.cloudera.com/content/cloudera/en/documentation/cloudera-kafka/latest/topics/kafka_performance.html">Performance and Resource Considerations</a> for a more complete discussion. In particular, note that <tt>replica.fetch.max.bytes</tt> would have to be set in the broker in order to <i>replicate</i> any very large messages. There is no per-topic equivalent to this.</p> 
 <div class="section"> 
  <h3 id="Automated_Kafka_Configuration">Automated Kafka Configuration</h3> 
  <p>The provided <tt>oryx-run.sh</tt> script can be used to print current configuration for Zookeeper, list existing topics in Kafka, and optionally create the configured input and update topics if needed. </p> 
  <p>You will need to create an Oryx configuration file first, which can be cloned from the example at <a class="externalLink" href="https://github.com/OryxProject/oryx/blob/master/app/conf/als-example.conf">conf/als-example.conf</a> as a start. At least change the Kafka and Zookeeper configuration, as well as topic names, as desired.</p> 
  <p>With this file as <tt>oryx.conf</tt> and any of the layer JAR files in the same directory, run:</p> 
  <div class="source"> 
   <div class="source"> 
    <pre>./oryx-run.sh kafka-setup

Input  ZK:    your-zk:2181
Input  Kafka: your-kafka:9092
Input  topic: OryxInput
Update ZK:    your-zk:2181
Update Kafka: your-kafka:9092
Update topic: OryxUpdate

All available topics:


Input topic OryxInput does not exist. Create it? y
Creating topic OryxInput
Created topic &quot;OryxInput&quot;.
Status of topic OryxInput:
Topic:OryxInput	PartitionCount:4	ReplicationFactor:1	Configs:
	Topic: OryxInput	Partition: 0	Leader: 120	Replicas: 120,121	Isr: 120,121
	Topic: OryxInput	Partition: 1	Leader: 121	Replicas: 121,120	Isr: 121,120
	Topic: OryxInput	Partition: 2	Leader: 120	Replicas: 120,121	Isr: 120,121
	Topic: OryxInput	Partition: 3	Leader: 121	Replicas: 121,120	Isr: 121,120

Update topic OryxUpdate does not exist. Create it? y
Creating topic OryxUpdate
Created topic &quot;OryxUpdate&quot;.
Updated config for topic &quot;OryxUpdate&quot;.
Status of topic OryxUpdate:
Topic:OryxUpdate	PartitionCount:1	ReplicationFactor:1	Configs:retention.ms=86400000,max.message.bytes=16777216
	Topic: OryxUpdate	Partition: 0	Leader: 120	Replicas: 120,121	Isr: 120,121
</pre> 
   </div> 
  </div> 
  <p>To watch messages sent to the input and update topics, to monitor action of the application, try:</p> 
  <div class="source"> 
   <div class="source"> 
    <pre>./oryx-run.sh kafka-tail
Input  ZK:    your-zk:2181
Input  Kafka: your-kafka:9092
Input  topic: OryxInput
Update ZK:    your-zk:2181
Update Kafka: your-kafka:9092
Update topic: OryxUpdate

...output...
</pre> 
   </div> 
  </div> 
  <p>Then in another window, you can feed input, such as the <tt>data.csv</tt> example from the <a href="endusers.html">end user docs</a>, into the input queue to verify it’s working with:</p> 
  <div class="source"> 
   <div class="source"> 
    <pre>./oryx-run.sh kafka-input --input-file data.csv
</pre> 
   </div> 
  </div> 
  <p>If all is well, these processes can be terminated. The cluster is ready to run Oryx.</p> 
 </div> 
</div> 
<div class="section"> 
 <h2 id="HDFS_and_Data_Layout">HDFS and Data Layout</h2> 
 <p>Kafka is the data transport mechanism in Oryx, so data is present in Kafka at least temporarily. However input data is also stored persistently in HDFS for later use. Likewise, models and updates are produced to a Kafka update topic, but models are also persisted to HDFS for later reference.</p> 
 <p>Input data is stored in HDFS under the directory defined by <tt>oryx.batch.storage.data-dir</tt>. Under this directory, subdirectories titled <tt>oryx-[timestamp].data</tt> are created, one for each batch executed by Spark Streaming in the Batch Layer. Here, <tt>timestamp</tt> is the familiar Unix timestamp in milliseconds.</p> 
 <p>Like most “files” output by distributed processes in Hadoop, this is actually a subdirectory containing many <tt>part-*</tt> files. Each file is a <tt>SequenceFile</tt>, where keys and values from the Kafka input topic have been serialized according to a <tt>Writable</tt> class implementation defined by <tt>oryx.batch.storage.key-writable-class</tt> and <tt>oryx.batch.storage.message-writable-class</tt>. By default, this is <tt>TextWritable</tt> and the string representation of keys and messages are recorded.</p> 
 <p>Data may be deleted from this data directory if desired. It will no longer be used in future Batch Layer computations. In particular, note that setting <tt>oryx.batch.storage.max-age-data-hours</tt> to a nonnegative value will configure the Batch Layer to automatically delete data older than the given number of hours.</p> 
 <p>Similarly, machine-learning-oriented applications (which extend <tt>MLUpdate</tt>) output the model chosen by the Batch Layer in each batch interval. It is also persisted in a subdirectory of the directory defined by <tt>oryx.batch.storage.model-dir</tt>. Under this directory are subdirectories named <tt>[timestamp]</tt>, where <tt>timestamp</tt> is again the familiar Unix timestamp in milliseconds.</p> 
 <p>The content of this subdirectory will depend on the application, but typically contains a PMML model called <tt>model.pmml</tt>, and optionally supplementary files that go with the model.</p> 
 <p>This directory exists to record PMML models for archiving and for use by other tools. Its content may be deleted if desired. Note also that setting <tt>oryx.batch.storage.max-age-model-hours</tt> to a nonnegative value will cause models older than the given number of hours to be deleted automatically.</p> 
 <h1 id="cloudera_quickstart_vm_setup">Cloudera Quickstart VM Setup</h1> 
 <p>For quick testing and evaluation, it may be useful to run a single-node cluster in a VM, like the <a class="externalLink" href="http://www.cloudera.com/content/www/en-us/downloads/quickstart_vms.html">Cloudera Quickstart VM</a>. This isn’t generally suitable for production use.</p> 
 <ul> 
  <li>Download and start the cluster VM 
   <ul> 
    <li>Give the VM at least 4 cores and 12GB of memory</li> 
    <li>Start the cluster in the VM; CDH Express is fine</li> 
   </ul></li> 
  <li>Install Java 8 (if not already the default) 
   <ul> 
    <li><tt>sudo yum install java-1.8.0-openjdk</tt> or similar to make a Java 8 implementation available</li> 
    <li>Launch Cloudera Manager, and log in to the UI (e.g. <tt>localhost:7180</tt>)</li> 
    <li>If any CDH services are running, stop them</li> 
    <li>From the “Hosts” menu, select “All Hosts”</li> 
    <li>Click the “Configuration” button</li> 
    <li>Under “Category” at the left, choose “Advanced”</li> 
    <li>In <tt>Java Home Directory</tt>, enter (for example): <tt>/usr/lib/jvm/jre-1.8.0-openjdk.x86_64/</tt></li> 
    <li>Click “Save Changes”</li> 
    <li>Restart the Cloudera Manager service from Cloudera Manager, or else just reboot the VM</li> 
   </ul></li> 
  <li>Configure the cluster 
   <ul> 
    <li>In parcel settings, add the location of Kafka parcels, currently <a class="externalLink" href="http://archive.cloudera.com/kafka/parcels/latest/">http://archive.cloudera.com/kafka/parcels/latest/</a></li> 
    <li>Distribute and activate the CDH parcel</li> 
    <li>Distribute and activate the Kafka parcel</li> 
    <li>Add Kafka as a service</li> 
    <li>Start services HDFS, Hue, Kafka, Spark, YARN, Zookeeper</li> 
    <li>In the Spark service, choose to add Spark user dir, and Spark app history dir</li> 
   </ul></li> 
  <li>Download the app files 
   <ul> 
    <li>Download the batch layer JAR, oryx-run.sh, compute-classpath.sh and example JAR file from the latest release at <a class="externalLink" href="https://github.com/OryxProject/oryx/releases">https://github.com/OryxProject/oryx/releases</a></li> 
    <li>Download the word count example config file from <a class="externalLink" href="https://github.com/OryxProject/oryx/blob/master/app/conf/wordcount-example.conf">https://github.com/OryxProject/oryx/blob/master/app/conf/wordcount-example.conf</a></li> 
   </ul></li> 
  <li>Run the batch layer 
   <ul> 
    <li>Use <tt>./oryx-run.sh kafka-setup --conf wordcount-example.conf</tt> to set up topics</li> 
    <li>Use <tt>./oryx-run.sh batch --conf wordcount-example.conf --app-jar example-....jar</tt> to start the batch layer</li> 
   </ul></li> 
  <li>Run other services as desired</li> 
 </ul> 
 <h1 id="handling_failure">Handling Failure</h1> 
 <p>Eventually, you’ll want to stop one or more of the Layers running, or restart it. Or maybe a server decides to die. What happens then? What’s the worst that can happen?</p> 
</div> 
<div class="section"> 
 <h2 id="Data_Loss">Data Loss</h2> 
 <p>Historical data is saved in HDFS, which should be configured for replication. HDFS ensures data is stored reliably. Kafka is also designed to cope with failure when configured to use replication.</p> 
 <p>That is, there is nothing special to do here in order to ensure that data is never completely lost. It is the job of HDFS and Kafka to always be available and not lose data.</p> 
</div> 
<div class="section"> 
 <h2 id="Server_Failure">Server Failure</h2> 
 <p>In general, all three Layer server processes should run continuously, and can and should be restarted immediately if they have to be stopped, or in case of a failure. This can be accomplished with an init script or similar mechanism (not included, yet).</p> 
 <div class="section"> 
  <h3 id="Serving_Layer">Serving Layer</h3> 
  <p>The Serving Layer has no state. On startup, it reads all models and updates available on the update topic. It begins answering queries as soon as any first, valid model is available. For this reason, it’s desirable to limit the retention time for the update topic.</p> 
  <p>The operation of the Serving Layer is not distributed. Each instance is independent, and may stop or start without affecting others.</p> 
 </div> 
 <div class="section"> 
  <h3 id="Speed_Layer">Speed Layer</h3> 
  <p>The Speed Layer also has no state, and also reads all models and updates available on the update topic. It begins producing updates as soon as it has a valid model. It also begins reading from the input topic, and at the moment, always reads from the latest offset.</p> 
  <p>The Speed Layer uses Spark Streaming and Spark for some of its computation. Spark has the responsibility of dealing with failures during computation in the cluster and retrying tasks.</p> 
  <p>Spark Streaming’s Kafka integration can in some cases recover from failure of the receiver that is reading from Kafka. If the entire process dies and is restarted, and <tt>oryx.id</tt> has been set, then reading will be able to resume from the last offset recorded by Kafka. (Otherwise, it will resume reading from the latest offset. This means data that arrived while no Speed Layer was running will not have produced any update.) Also, data that arrives before the Speed Layer has a model is ignored too. It effectively adopts “at most once” semantics.</p> 
  <p>Because the role of the Speed Layer is to provide an approximate, “best effort” update to the last published model, this behavior is generally no problem, and desirable because of its simplicity.</p> 
 </div> 
 <div class="section"> 
  <h3 id="Batch_Layer">Batch Layer</h3> 
  <p>The Batch Layer is the most complex, since it does generate some state:</p> 
  <ul> 
   <li>Historical data, is always persisted to HDFS</li> 
   <li>If the app chooses to, additional state like models can be persisted to HDFS as well as topics</li> 
  </ul> 
  <p>It also is most sensitive to reading data multiple times or not at all, since it is the component that creates the “official” next model.</p> 
  <p>As with the Speed Layer, Spark and Spark Streaming handles many of the failure scenarios during computation. It also manages storing data to HDFS and is responsible for avoiding writing the same data twice.</p> 
  <p>Applications are responsible for recovering their own ‘state’; currently, applications built on the Oryx ML tier write state into unique subdirectories, and will simply produce a new set of state in a new directory when restarted. Previous state, if it exists, will have been completely written or not at all.</p> 
  <p>The Batch Layer also currently adopts the same “at most once” semantics as the Speed Layer. As above, if the entire process dies and is restarted, and <tt>oryx.id</tt> has been set, then reading will be able to resume from the last offset recorded by Kafka, and otherwise, it will resume reading from the latest offset.</p> 
  <h1 id="troubleshooting__faq">Troubleshooting / FAQ</h1> 
 </div> 
</div> 
<div class="section"> 
 <h2 id="Unsupported_majorminor_version_520">Unsupported major.minor version 52.0</h2> 
 <p>This means you are running 7 or earlier somewhere. Oryx 2.2 requires Java 8 or later. See section above on installing Java 8 and making it available everywhere on the cluster.</p> 
</div> 
<div class="section"> 
 <h2 id="Initial_job_has_not_accepted_any_resources">Initial job has not accepted any resources</h2> 
 <p>The error usually means that your YARN cluster can’t allocate the resources (memory, cores) that your application is requesting. You’ll have to check and increase what YARN can allocate, free up room, or decrease the amount that your app asks for.</p> 
 <p>The relevant YARN settings are:</p> 
 <ul> 
  <li>Container Memory (<tt>yarn.nodemanager.resource.memory-mb</tt>) - the maximum memory that one YARN node has to allocate to containers</li> 
  <li>Container Virtual CPU Cores (<tt>yarn.nodemanager.resource.cpu-vcores</tt>) - same, for cores</li> 
  <li>Container Memory Maximum (<tt>yarn.scheduler.maximum-allocation-mb</tt>) - maximum memory for one container</li> 
  <li>Container Virtual CPU Cores Maximum (<tt>yarn.scheduler.maximum-allocation-vcores</tt>) - maximum cores for one container</li> 
 </ul> 
 <p>The relevant app settings are:</p> 
 <ul> 
  <li><tt>oryx.{batch,speed}.streaming.num-executors</tt> - number of executors (YARN containers) to allocate</li> 
  <li><tt>oryx.{batch,speed}.streaming.executor-cores</tt> - cores to allocate per executor</li> 
  <li><tt>oryx.{batch,speed}.streaming.executor-memory</tt> - memory to allocate per executor</li> 
 </ul> 
</div> 
<div class="section"> 
 <h2 id="Required_executor_memory__MB_is_above_the_max_threshold__MB_of_this_cluster">Required executor memory (… MB) is above the max threshold (… MB) of this cluster</h2> 
 <p>This means your YARN configuration limits the maximum container size that can be requested. Increase the Container Memory Maximum (<tt>yarn.scheduler.maximum-allocation-mb</tt>) to something larger. For Spark, it generally makes sense to allow large containers.</p> 
</div> 
<div class="section"> 
 <h2 id="IllegalArgumentException_Wrong_FS">IllegalArgumentException: Wrong FS</h2> 
 <div class="source"> 
  <div class="source"> 
   <pre>java.lang.IllegalArgumentException: Wrong FS: hdfs:..., expected: file:///
    	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)
</pre> 
  </div> 
 </div> 
 <p>This typically means you are using HDFS, but your Hadoop config (e.g. <tt>core-site.xml</tt>, typically in <tt>/etc/hadoop/conf</tt> is not on the classpath. If you’re building a custom <tt>compute-classpath.sh</tt> script make sure to include this directory along with JARs.</p> 
</div> 
<div class="section"> 
 <h2 id="I_need_to_purge_all_previous_data_and_start_again">I need to purge all previous data and start again</h2> 
 <p>Input data exists in the input Kafka topic for a time before being copied into HDFS. So, input potentially exists as unread message in this topic as well as in the HDFS directory defined by <tt>orxy.batch.storage.data-dir</tt>. It’s easy to delete the data in HDFS; it’s harder to ensure older data in the input topic is not read.</p> 
 <p>The simplest solution is to create a new input topic and change configuration to use it. Then, also delete any pre-existing data in HDFS (or use a new directory). Similarly, since the update topic is read from the beginning, it’s easiest to make a new update topic instead.</p> 
 <p>It’s possible to reuse an existing topic name, by removing all its data (difficult, not recommended) or simply deleting and recreating it. If recreating the topic, it’s necessary to reset the consumer offset Oryx will use. This can be done by directly manipulating offsets stored in Zookeeper, to delete them (somewhat hard, not recommended), or by simply switching <tt>oryx.id</tt> to another value.</p> 
</div> 
<div class="section"> 
 <h2 id="Speed_Layer_isnt_producing_updates_but_is_running">Speed Layer isn’t producing updates, but is running</h2> 
 <p>The Speed Layer won’t produce updates until it has loaded a model. Also, check if the Speed Layer’s batches are queued up. If batches are being created faster than they’re processed, then each is waiting longer and longer to start processing, delaying their updates.</p> 
 <h1 id="performance">Performance</h1> 
 <p>See <a href="performance.html">the performance doc</a>.</p> 
 <h1 id="differences_from_oryx_1">Differences from Oryx 1</h1> 
 <p>Design goals for Oryx 2 were:</p> 
 <ul> 
  <li>Provide a more reusable platform for <a class="externalLink" href="http://lambda-architecture.net/">lambda-architecture</a>-style designs, with batch, speed and serving layers</li> 
  <li>Make each layer usable independently</li> 
  <li>Better support for common machine learning needs</li> 
  <li>Test/train set split and evaluation</li> 
  <li>Parallel model build</li> 
  <li>Hyper-parameter selection</li> 
  <li>Use newer technologies like Spark and Streaming in order to simplify:</li> 
  <li>Remove separate in-core implementations for scale-down</li> 
  <li>Remove custom data transport implementation in favor of <a class="externalLink" href="http://kafka.apache.org/">Apache Kafka</a></li> 
  <li>Use a ‘real’ streaming framework instead of reimplementing a simple one</li> 
  <li>Remove complex MapReduce-based implementations in favor of <a class="externalLink" href="http://spark.apache.org/">Apache Spark</a>-based implementations</li> 
  <li>Support more input (i.e. not just <a class="externalLink" href="http://en.wikipedia.org/wiki/Comma-separated_values">CSV</a>)</li> 
 </ul> 
</div> 
<div class="section"> 
 <h2 id="Architecture_Differences">Architecture Differences</h2> 
 <table border="0" class="bodyTable table table-striped table-hover"> 
  <thead> 
   <tr class="a"> 
    <th>Oryx 1 </th> 
    <th>Oryx 2 </th> 
   </tr> 
  </thead> 
  <tbody> 
   <tr class="b"> 
    <td>One monolithic “tier” for lambda architecture and apps </td> 
    <td>Three tiers: lambda, ML, apps </td> 
   </tr> 
   <tr class="a"> 
    <td>No app-level extensibility </td> 
    <td>Platform for building other lambda- and ML-based apps </td> 
   </tr> 
   <tr class="b"> 
    <td>Two layers: Computation and Serving </td> 
    <td>Three layers: Batch, Speed and Serving </td> 
   </tr> 
   <tr class="a"> 
    <td>Based on Crunch, MapReduce, HDFS, Tomcat </td> 
    <td>Based on HDFS, YARN, Spark (+ Streaming, MLlib), Kafka, Zookeeper, Tomcat </td> 
   </tr> 
   <tr class="b"> 
    <td>32K lines production code / 3K test </td> 
    <td>19K lines production code / 9K test: simpler, better tested </td> 
   </tr> 
  </tbody> 
 </table> 
</div> 
<div class="section"> 
 <h2 id="Deployment_Differences">Deployment Differences</h2> 
 <table border="0" class="bodyTable table table-striped table-hover"> 
  <thead> 
   <tr class="a"> 
    <th>Oryx 1 </th> 
    <th>Oryx 2 </th> 
   </tr> 
  </thead> 
  <tbody> 
   <tr class="b"> 
    <td>Requires Java 6, optionally core Hadoop 2.2+ (including “MR1”) </td> 
    <td>Requires Java 8, core Hadoop 2.6+ (YARN, not “MR1”) Spark 1.6+, Kafka 0.9+, Zookeeper 3.4.5+ </td> 
   </tr> 
   <tr class="a"> 
    <td>Supports local, non-Hadoop deployment </td> 
    <td>No non-Hadoop deployment </td> 
   </tr> 
   <tr class="b"> 
    <td>Supports MapReduce-based Hadoop deployment </td> 
    <td>Supports only deployment with core Hadoop, YARN, Spark, Kafka </td> 
   </tr> 
  </tbody> 
 </table> 
</div> 
<div class="section"> 
 <h2 id="Scale_and_Reliability_Differences">Scale and Reliability Differences</h2> 
 <table border="0" class="bodyTable table table-striped table-hover"> 
  <thead> 
   <tr class="a"> 
    <th>Oryx 1 </th> 
    <th>Oryx 2 </th> 
   </tr> 
  </thead> 
  <tbody> 
   <tr class="b"> 
    <td>Memory-efficient </td> 
    <td>Fast, memory-hungry </td> 
   </tr> 
   <tr class="a"> 
    <td>Custom, best-effort data transport between layers </td> 
    <td>Reliable data transport via Kafka </td> 
   </tr> 
   <tr class="b"> 
    <td>Custom MapReduce-based algorithm implementations in Computation Layer </td> 
    <td>Spark Streaming-based batch layer framework and Spark MLlib-based algorithm implementations </td> 
   </tr> 
   <tr class="a"> 
    <td>Custom in-core incremental model update (“speed layer”) </td> 
    <td>Spark Streaming-based distributed model update </td> 
   </tr> 
  </tbody> 
 </table> 
</div> 
<div class="section"> 
 <h2 id="Migration">Migration</h2> 
 <p>The bad news is that no direct migration is possible between Oryx 1 and Oryx 2; they have very different implementations. However, differences in the user- and developer-facing aspects are by design similar or identical.</p> 
 <div class="section"> 
  <h3 id="REST_API">REST API</h3> 
  <p>Oryx 2 contains the same set of end-to-end ML applications as Oryx 1, and exposes virtually the same REST API, unchanged. The only significant difference is that there is no longer a <tt>/refresh</tt> endpoint, because it is unnecessary.</p> 
 </div> 
 <div class="section"> 
  <h3 id="Configuration">Configuration</h3> 
  <p>Both implementations use a single configuration file parsed by Typesafe Config. The property namespaces are different but there are some similarities. Compare the <a class="externalLink" href="https://github.com/cloudera/oryx/blob/master/common/src/main/resources/reference.conf">Oryx 1 configuration</a> to the <a class="externalLink" href="https://github.com/OryxProject/oryx/blob/master/framework/oryx-common/src/main/resources/reference.conf">Oryx 2 configuration</a> to understand some of the correspondence and difference.</p> 
 </div> 
 <div class="section"> 
  <h3 id="Data_Storage_and_Transport">Data Storage and Transport</h3> 
  <p>In Oryx 1, all data was stored in a series of directories in HDFS. In Oryx 2, data is transported via Kafka (which ultimately stores data in HDFS) and in HDFS as managed by a Spark Streaming process. Although it is still possible to side-load data files via HDFS in Oryx 2, it is not supported and is discouraged, in favor of sending data directly to a Kafka queue.</p> 
 </div> 
 <div class="section"> 
  <h3 id="Data_Formats">Data Formats</h3> 
  <p>In theory, the framework is agnostic to data types and encodings passed between layers. In practice, the provided applications consume the same CSV-encoded data format as Oryx 1.</p> 
 </div> 
 <div class="section"> 
  <h3 id="Deployment">Deployment</h3> 
  <p>The deployment requirements are the most different. Although all layers are still distributed as Java <tt>.jar</tt> binaries, now, a Hadoop cluster is required, including HDFS, YARN, Kafka, Spark, and Zookeeper services. Your environment or cluster must be updated to include these services before you can use Oryx 2.</p> 
 </div> 
</div>
			</div>
		</div>
		<div class="span4">
			<div id="toc-sidebar">
				<div class="well">
					<ul class="nav nav-list">
						<li class="nav-header">Table of Contents</li>
		<li class="dropdown"><a href="#cluster_setup" title="Cluster Setup">Cluster Setup <b class="caret"></b></a>
			<ul class="nav nav-list">
		<li><a href="#Deployment_Architecture" title="Deployment Architecture">Deployment Architecture</a>
		<li><a href="#Services" title="Services">Services</a>
		<li><a href="#Java" title="Java">Java</a>
		<li class="dropdown"><a href="#Configuring_Kafka" title="Configuring Kafka">Configuring Kafka <b class="caret"></b></a>
			<ul class="nav nav-list">
		<li><a href="#Automated_Kafka_Configuration" title="Automated Kafka Configuration">Automated Kafka Configuration</a>
				<li class="divider"></li>
			</ul>
		</li>
		<li><a href="#HDFS_and_Data_Layout" title="HDFS and Data Layout">HDFS and Data Layout</a>
				<li class="divider"></li>
			</ul>
		</li>
		<li><a href="#cloudera_quickstart_vm_setup" title="Cloudera Quickstart VM Setup">Cloudera Quickstart VM Setup</a>
		<li class="dropdown"><a href="#handling_failure" title="Handling Failure">Handling Failure <b class="caret"></b></a>
			<ul class="nav nav-list">
		<li><a href="#Data_Loss" title="Data Loss">Data Loss</a>
		<li class="dropdown"><a href="#Server_Failure" title="Server Failure">Server Failure <b class="caret"></b></a>
			<ul class="nav nav-list">
		<li><a href="#Serving_Layer" title="Serving Layer">Serving Layer</a>
		<li><a href="#Speed_Layer" title="Speed Layer">Speed Layer</a>
		<li><a href="#Batch_Layer" title="Batch Layer">Batch Layer</a>
				<li class="divider"></li>
			</ul>
		</li>
			</ul>
		</li>
		<li class="dropdown"><a href="#troubleshooting__faq" title="Troubleshooting / FAQ">Troubleshooting / FAQ <b class="caret"></b></a>
			<ul class="nav nav-list">
		<li><a href="#Unsupported_majorminor_version_520" title="Unsupported major.minor version 52.0">Unsupported major.minor version 52.0</a>
		<li><a href="#Initial_job_has_not_accepted_any_resources" title="Initial job has not accepted any resources">Initial job has not accepted any resources</a>
		<li><a href="#Required_executor_memory__MB_is_above_the_max_threshold__MB_of_this_cluster" title="Required executor memory (… MB) is above the max threshold (… MB) of this cluster">Required executor memory (… MB) is above the max threshold (… MB) of this cluster</a>
		<li><a href="#IllegalArgumentException_Wrong_FS" title="IllegalArgumentException: Wrong FS">IllegalArgumentException: Wrong FS</a>
		<li><a href="#I_need_to_purge_all_previous_data_and_start_again" title="I need to purge all previous data and start again">I need to purge all previous data and start again</a>
		<li><a href="#Speed_Layer_isnt_producing_updates_but_is_running" title="Speed Layer isn’t producing updates, but is running">Speed Layer isn’t producing updates, but is running</a>
				<li class="divider"></li>
			</ul>
		</li>
		<li><a href="#performance" title="Performance">Performance</a>
		<li class="dropdown"><a href="#differences_from_oryx_1" title="Differences from Oryx 1">Differences from Oryx 1 <b class="caret"></b></a>
			<ul class="nav nav-list">
		<li><a href="#Architecture_Differences" title="Architecture Differences">Architecture Differences</a>
		<li><a href="#Deployment_Differences" title="Deployment Differences">Deployment Differences</a>
		<li><a href="#Scale_and_Reliability_Differences" title="Scale and Reliability Differences">Scale and Reliability Differences</a>
		<li class="dropdown"><a href="#Migration" title="Migration">Migration <b class="caret"></b></a>
			<ul class="nav nav-list">
		<li><a href="#REST_API" title="REST API">REST API</a>
		<li><a href="#Configuration" title="Configuration">Configuration</a>
		<li><a href="#Data_Storage_and_Transport" title="Data Storage and Transport">Data Storage and Transport</a>
		<li><a href="#Data_Formats" title="Data Formats">Data Formats</a>
		<li><a href="#Deployment" title="Deployment">Deployment</a>
				<li class="divider"></li>
			</ul>
		</li>
			</ul>
		</li>
					</ul>
				</div>
			</div>
		</div>
	</div>
	</div>

	</div><!-- /container -->

	<!-- Footer
	================================================== -->
	<footer class="well">
		<div class="container">
			<div class="row">
				<div class="span9 bottom-nav">
					<ul class="nav nav-list">
					</ul>
				</div>
			</div>
		</div>
	</footer>

	<div class="container subfooter">
		<div class="row">
			<div class="span12">
				<p class="pull-right"><a href="#">Back to top</a></p>
				<p class="copyright">Copyright &copy;2014-2016. All Rights Reserved.</p>
				<p><a href="http://github.com/andriusvelykis/reflow-maven-skin" title="Reflow Maven skin">Reflow Maven skin</a> by <a href="http://andrius.velykis.lt" target="_blank" title="Andrius Velykis">Andrius Velykis</a>.</p>
			</div>
		</div>
	</div>

	<!-- Le javascript
	================================================== -->
	<!-- Placed at the end of the document so the pages load faster -->
	<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>

	<script src="http://netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
	<script src="../js/lightbox.min.js"></script>
	<script src="../js/reflow-scroll.js"></script>

	<script src="../js/reflow-skin.js"></script>

	</body>
</html>
