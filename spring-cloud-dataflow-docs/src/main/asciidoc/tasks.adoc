[[spring-cloud-task]]
= Tasks

[partintro]
--
This section goes into more detail about how you can work with
http://cloud.spring.io/spring-cloud-task/[Spring Cloud Task]. It covers topics such as
creating and running task applications.

If you're just starting out with Spring Cloud Data Flow, you should probably read the
_<<getting-started.adoc#getting-started, Getting Started>>_ guide before diving into
this section.
--

[[spring-cloud-dataflow-task-intro]]
== Introducing Spring Cloud Task
A task executes a process on demand.  In this case a task is a
http://projects.spring.io/spring-boot/[Spring Boot] application that is annotated with
`@EnableTask`.  Hence a user launches a task that performs a certain process, and once
complete the task ends. An example of a task would be a boot application that exports
data from a JDBC repository to an HDFS instance.  Tasks record the start time and the end
time as well as the boot exit code in a relational database. The task implementation is
based on the http://cloud.spring.io/spring-cloud-task/[Spring Cloud Task] project.

== The Lifecycle of a task
Before we dive deeper into the details of creating Tasks, we need to understand the
typical lifecycle for tasks in the context of Spring Cloud Data Flow:

1. Register a Task App
2. Create a Task Definition
3. Launch a Task
4. Task Execution
5. Destroy a Task Definition

=== Creating a custom Task Application
While Spring Cloud Task does provide a number of out of the box applications (via the
https://github.com/spring-cloud-task-app-starters[spring-cloud-task-app-starters]),
most task applications will be custom developed.  In order to create a custom task application:

. Create a new project via http://start.spring.io[Spring Initializer] via either the
website or your IDE making sure to select the following starters:
.. `Cloud Task` - This dependency is the `spring-cloud-starter-task`.
.. `JDBC` - This is the dependency for the `spring-jdbc` starter.
. Within your new project, create a new class that will serve as your main class:

```
@EnableTask
@SpringBootApplication
public class MyTask {

    public static void main(String[] args) {
		SpringApplication.run(MyTask.class, args);
	}
}
```
[start=3]
. With this, you'll need one or more `CommandLineRunner` or `ApplicationRunner` within
your application.  You can either implement your own or use the ones provided by Spring
Boot (there is one for running batch jobs for example).
. Packaging your application up via Spring Boot into an Ã¼ber jar is done via the standard
  Boot conventions.
.  The packaged application can be registered and deployed as noted below.

=== Registering a Task Application
Register a Task App with the App Registry using the Spring Cloud Data Flow Shell
`app register` command. You must provide a unique name and a URI that can be
resolved to the app artifact. For the type, specify "task". Here are a few examples:

```
dataflow:>app register --name task1 --type task --uri maven://com.example:mytask:1.0.2

dataflow:>app register --name task2 --type task --uri file:///Users/example/mytask-1.0.2.jar

dataflow:>app register --name task3 --type task --uri http://example.com/mytask-1.0.2.jar
```

When providing a URI with the `maven` scheme, the format should conform to the following:

```
maven://<groupId>:<artifactId>[:<extension>[:<classifier>]]:<version>
```

If you would like to register multiple apps at one time, you can store them in a properties file
where the keys are formatted as `<type>.<name>` and the values are the URIs. For example, this
would be a valid properties file:

```
task.foo=file:///tmp/foo.jar
task.bar=file:///tmp/bar.jar
```

Then use the `app import` command and provide the location of the properties file via `--uri`:

```
app import --uri file:///tmp/task-apps.properties
```
For convenience, we have the static files with application-URIs (for both maven and docker) available for all the out-of-the-box
Task app-starters. You can point to this file and import all the application-URIs in bulk. Otherwise, as explained in
previous paragraphs, you can register them individually or have your own custom property file with only the required application-URIs
in it. It is recommended, however, to have a "focused" list of desired application-URIs in a custom property file.


List of available static property files:

[width="100%",frame="topbot",options="header"]
|======================
|Artifact Type |Stable Release |SNAPSHOT Release
|Maven   |link:http://bit.ly/Belmont-GA-task-applications-maven[http://bit.ly/Belmont-GA-task-applications-maven] |link:http://bit.ly/Belmont-BUILD-SNAPSHOT-task-applications-maven[http://bit.ly/Belmont-BUILD-SNAPSHOT-task-applications-maven]
|Docker  |link:http://bit.ly/Belmont-GA-task-applications-docker[http://bit.ly/Belmont-GA-task-applications-docker] | link:http://bit.ly/Belmont-BUILD-SNAPSHOT-task-applications-docker[http://bit.ly/Belmont-BUILD-SNAPSHOT-task-applications-docker]
|======================

For example, if you would like to register all out-of-the-box task applications in bulk, you can with
the following command.

```
dataflow:>app import --uri http://bit.ly/Belmont-GA-task-applications-maven
```

You can also pass the `--local` option (which is TRUE by default) to indicate whether the
properties file location should be resolved within the shell process itself. If the location should
be resolved from the Data Flow Server process, specify `--local false`.

When using either `app register` or `app import`, if a task app is already registered with
the provided name, it will not be overridden by default. If you would like to override the
pre-existing task app, then include the `--force` option.

[NOTE]
In some cases the Resource is resolved on the server side, whereas in others the
URI will be passed to a runtime container instance where it is resolved. Consult
the specific documentation of each Data Flow Server for more detail.


=== Creating a Task
Create a Task Definition from a Task App by providing a definition name as well as
properties that apply to the task execution.  Creating a task definition can be done via
the restful API or the shell.  To create a task definition using the shell, use the
`task create` command to create the task definition.  For example:

```
dataflow:>task create mytask --definition "timestamp --format=\"yyyy\""
 Created new task 'mytask'
```

A listing of the current task definitions can be obtained via the restful API or the
shell.  To get the task definition list using the shell, use the `task list` command.

=== Launching a Task
An adhoc task can be launched via the restful API or via the shell.  To launch an ad-hoc
task via the shell use the `task launch` command.  For example:

```
dataflow:>task launch mytask
 Launched task 'mytask'
```

When a task is launched, any properties that need to be passed as the command line arguments
to the task application can be set when launching the task as follows:

```
dataflow:>task launch mytask --arguments "--server.port=8080,--foo=bar"
```

Additional properties meant for a `TaskLauncher` itself can be passed
in using a `--properties` option. Format of this option is a comma
delimited string of properties prefixed with `app.<task definition
name>.<property>`. Properties are passed
to `TaskLauncher` as application properties and it is up to an
implementation to choose how those are passed into an actual task
application. If the property is prefixed with `deployer` instead of `app` it is
passed to `TaskLauncher` as a deployment property and its meaning may
be `TaskLauncher` implementation specific.

```
dataflow:>task launch mytask --properties "deployer.timestamp.foo1=bar1,app.timestamp.foo2=bar2"
```

==== Common application properties

In addition to configuration via DSL, Spring Cloud Data Flow provides a mechanism for setting common properties to all
the task applications that are launched by it.
This can be done by adding properties prefixed with `spring.cloud.dataflow.applicationProperties.task` when starting the server.
When doing so, the server will pass all the properties, without the prefix, to the instances it launches.

For example, all the launched applications can be configured to use the properties `foo` and `fizz` by launching the Data Flow server
with the following options:

```
--spring.cloud.dataflow.applicationProperties.task.foo=bar
--spring.cloud.dataflow.applicationProperties.task.fizz=bar2
```

This will cause the properties `foo=bar` and `fizz=bar2` to be passed to all the launched applications.

[NOTE]
Properties configured using this mechanism have lower precedence than task deployment properties.
They will be overridden if a property with the same key is specified at task launch time (e.g. `app.trigger.fizz`
will override the common property).


=== Reviewing Task Executions
Once the task is launched the state of the task is stored in a relational DB.  The state
includes:

* Task Name
* Start Time
* End Time
* Exit Code
* Exit Message
* Last Updated Time
* Parameters

A user can check the status of their task executions via the restful API or by the shell.
To display the latest task executions via the shell use the `task execution list` command.

To get a list of task executions for just one task definition, add `--name` and
the task definition name, for example `task execution list --name foo`.  To retrieve full
details for a task execution use the `task display` command with the id of the task execution,
for example `task display --id 549`.

=== Destroying a Task
Destroying a Task Definition will remove the definition from the definition repository.
This can be done via the restful API or via the shell.  To destroy a task via the shell
use the `task destroy` command. For example:

```
dataflow:>task destroy mytask
 Destroyed task 'mytask'
```

The task execution information for previously launched tasks for the definition will
remain in the task repository.

NOTE: This will not stop any currently executing tasks for this definition, instead it just
removes the task definition from the database.

[[spring-cloud-dataflow-task-repository]]
== Task Repository

Out of the box Spring Cloud Data Flow offers an embedded instance of the H2 database.
The H2 is good for development purposes but is not recommended for production use.

=== Configuring the Task Execution Repository
To add a driver for the database that will store the Task Execution information, a
dependency for the driver will need to be added to a maven pom file and the
Spring Cloud Data Flow will need to be rebuilt.  Since Spring Cloud Data Flow is comprised of an SPI for
each environment it supports, please review the SPI's documentation on which POM should be
updated to add the dependency and how to build.  This document will cover how to setup the
dependency for local SPI.

==== Local

1. Open the spring-cloud-dataflow-server-local/pom.xml in your IDE.
2. In the `dependencies` section add the dependency for the database driver required.  In
the sample below postgresql has been chosen.
```
<dependencies>
...
    <dependency>
        <groupId>org.postgresql</groupId>
        <artifactId>postgresql</artifactId>
    </dependency>
...
</dependencies>
```
[start=3]
1. Save the changed pom.xml
2. Build the application as described here: <<appendix-building.adoc#building, Building Spring Cloud Data Flow>>

==== Task Application Repository

When launching a task application be sure that the database driver that is being
used by Spring Cloud Data Flow is also a dependency on the task application. For
example if your Spring Cloud Data Flow is set to use Postgresql, be sure that
the task application _also_ has Postgresql as a dependency.

NOTE: When executing tasks externally (i.e. command line) and you wish for
Spring Cloud Data Flow to show the TaskExecutions in its UI, be sure that
common datasource settings are shared among the both. By default
Spring Cloud Task will use a local H2 instance and the execution will
not be recorded to the database used by Spring Cloud Data Flow.

=== Datasource

To configure the datasource Add the following properties to the dataflow-server.yml or via
environment variables:

a. spring.datasource.url
b. spring.datasource.username
c. spring.datasource.password
d. spring.datasource.driver-class-name

For example adding postgres would look something like this:

* Environment variables:
```
export spring_datasource_url=jdbc:postgresql://localhost:5432/mydb
export spring_datasource_username=myuser
export spring_datasource_password=mypass
export spring_datasource_driver-class-name="org.postgresql.Driver"
```
* dataflow-server.yml
```
spring:
  datasource:
    url: jdbc:postgresql://localhost:5432/mydb
    username: myuser
    password: mypass
    driver-class-name:org.postgresql.Driver
```

[[spring-cloud-dataflow-task-events]]
== Subscribing to Task/Batch Events

You can also tap into various task/batch events when the task is launched.
If the task is enabled to generate task and/or batch events (with the additional dependencies `spring-cloud-task-stream` and `spring-cloud-stream-binder-kafka`, in the case of Kafka as the binder), those events are published during the task lifecycle.
By default, the destination names for those published events on the broker (rabbit, kafka etc.,) are the event names themselves (for instance: `task-events`, `job-execution-events` etc.,).

```
dataflow:>task create myTask --definition âmyBatchJob"
dataflow:>task launch myTask
dataflow:>stream create task-event-subscriber1 --definition ":task-events > log" --deploy
```

You can control the destination name for those events by specifying explicit names when launching the task such as:

```
dataflow:>task launch myTask --properties "spring.cloud.stream.bindings.task-events.destination=myTaskEvents"
dataflow:>stream create task-event-subscriber2 --definition ":myTaskEvents > log" --deploy
```

The default Task/Batch event and destination names on the broker are enumerated below:

.Task/Batch Event Destinations

[cols="2*"]
|===

|*Event*|*Destination*

|Task events
|`task-events`
|Job Execution events  |`job-execution-events`
|Step Execution events|`step-execution-events`
|Item Read events|`item-read-events`
|Item Process events|`item-process-events`
|Item Write events|`item-write-events`
|Skip events|`skip-events`
|===

[[spring-cloud-dataflow-launch-tasks-from-stream]]
== Launching Tasks from a Stream

You can launch a task from a stream by using one of the available `task-launcher` sinks. Currently the platforms supported
via the `task-launcher` sinks are
https://github.com/spring-cloud-stream-app-starters/tasklauncher-local[local],
https://github.com/spring-cloud-stream-app-starters/tasklauncher-cloudfoundry[Cloud Foundry], and
https://github.com/spring-cloud-stream-app-starters/tasklauncher-yarn[Yarn].

NOTE: `task-launcher-local` is meant for development purposes only.

A `task-launcher` sink expects a message containing a https://github.com/spring-cloud/spring-cloud-task/blob/master/spring-cloud-task-stream/src/main/java/org/springframework/cloud/task/launcher/TaskLaunchRequest.java[TaskLaunchRequest] object in its payload. From the `TaskLaunchRequest` object the `task-launcher` will obtain the URI of the artifact to be launched as well as the environment properties, command line arguments, deployment properties and application name to be used by the task.

The https://github.com/spring-cloud-stream-app-starters/tasklauncher-local/blob/v1.2.0.RELEASE/spring-cloud-starter-stream-sink-task-launcher-local/README.adoc[task-launcher-local] can be added to the available sinks by executing the app register command as follows (for the Rabbit Binder):

```
app register --name task-launcher-local --type sink --uri maven://org.springframework.cloud.stream.app:task-launcher-local-sink-rabbit:jar:1.2.0.RELEASE
```

In the case of a maven based task that is to be launched, the `task-launcher` application is responsible for downloading the artifact.  You *must* configure the `task-launcher` with the appropriate configuration of https://github.com/spring-cloud/spring-cloud-deployer/blob/master/spring-cloud-deployer-resource-maven/src/main/java/org/springframework/cloud/deployer/resource/maven/MavenProperties.java[Maven Properties] such as `--maven.remote-repositories.repo1.url=http://repo.spring.io/libs-milestone"` to resolve artifacts, in this case against a milestone repo.  Note that this repo can be different than the one used to register the `task-launcher` application itself.

=== TriggerTask

One way to launch a task using the `task-launcher` is to use the https://github.com/spring-cloud-stream-app-starters/triggertask/blob/v1.2.0.RELEASE/spring-cloud-starter-stream-source-triggertask/README.adoc[triggertask] source. The `triggertask` source
will emit a message with a `TaskLaunchRequest` object containing the required launch information.
The `triggertask` can be added to the available sources by executing the app register command as follows (for the Rabbit Binder):

```
app register --type source --name triggertask --uri maven://org.springframework.cloud.stream.app:triggertask-source-rabbit:1.2.0.RELEASE
```

An example of this would be to launch the timestamp task once every 60 seconds, the stream to implement this would look like:

```
stream create foo --definition "triggertask --triggertask.uri=maven://org.springframework.cloud.task.app:timestamp-task:jar:1.2.0.RELEASE --trigger.fixed-delay=60 --triggertask.environment-properties=spring.datasource.url=jdbc:h2:tcp://localhost:19092/mem:dataflow,spring.datasource.username=sa | task-launcher-local --maven.remote-repositories.repo1.url=http://repo.spring.io/libs-release" --deploy
```

If you execute `runtime apps` you can find the log file for the task launcher sink. Tailing that file you can find the log file for the launched tasks. The setting of `triggertask.environment-properties` is so that all the task executions can be collected in the same H2 database used in the local version of the Data Flow Server.  You can then see the list of task executions using the shell command `task execution list`

```
dataflow:>task execution list
ââââââââââââââââââââââ¤âââ¤âââââââââââââââââââââââââââââ¤âââââââââââââââââââââââââââââ¤ââââââââââ
â     Task Name      âIDâ         Start Time         â          End Time          âExit Codeâ
â âââââââââââââââââââââªâââªâââââââââââââââââââââââââââââªâââââââââââââââââââââââââââââªââââââââââ£
âtimestamp-task_26176â4 âTue May 02 12:13:49 EDT 2017âTue May 02 12:13:49 EDT 2017â0        â
âtimestamp-task_32996â3 âTue May 02 12:12:49 EDT 2017âTue May 02 12:12:49 EDT 2017â0        â
âtimestamp-task_58971â2 âTue May 02 12:11:50 EDT 2017âTue May 02 12:11:50 EDT 2017â0        â
âtimestamp-task_13467â1 âTue May 02 12:10:50 EDT 2017âTue May 02 12:10:50 EDT 2017â0        â
ââââââââââââââââââââââ§âââ§âââââââââââââââââââââââââââââ§âââââââââââââââââââââââââââââ§ââââââââââ
```

=== TaskLaunchRequest-transform

Another option to start a task using the `task-launcher` would be to create a stream using the
https://github.com/spring-cloud-stream-app-starters/tasklaunchrequest-transform[Tasklaunchrequest-transform] processor to translate a message payload to a `TaskLaunchRequest`.

The `tasklaunchrequest-transform` can be added to the available processors by executing the app register command as follows (for the Rabbit Binder):

```
app register --type processor --name tasklaunchrequest-transform --uri maven://org.springframework.cloud.stream.app:tasklaunchrequest-transform-processor-rabbit:1.2.0.RELEASE
```

For example:

```
stream create task-stream --definition "http --port=9000 | tasklaunchrequest-transform --uri=maven://org.springframework.cloud.task.app:timestamp-task:jar:1.2.0.RELEASE | task-launcher-local --maven.remote-repositories.repo1.url=http://repo.spring.io/libs-release"
```

[[spring-cloud-dataflow-composed-tasks]]
== Composed Tasks

Spring Cloud Data Flow allows a user to create a directed graph where each node
of the graph is a task application.  This is done by using the DSL for composed
tasks.  A composed task can be created via the RESTful API, the Spring Cloud
Data Flow Shell, or the Spring Cloud Data Flow UI.

=== Configuring the Composed Task Runner in Spring Cloud Data Flow

Composed tasks are executed via a task application called the https://github.com/spring-cloud-task-app-starters/composed-task-runner[Composed Task Runner].

==== Registering the Composed Task Runner application

Out of the box the Composed Task Runner application is not registered with Spring Cloud Data Flow. So, to launch composed tasks we must first register the Composed
Task Runner as an application with Spring Cloud Data Flow as follows:

```
app register --name composed-task-runner --type task --uri maven://org.springframework.cloud.task.app:composedtaskrunner-task:<DESIRED_VERSION>
```

You can also configure Spring Cloud Data Flow to use a different task definition
name for the composed task runner.  This can be done by setting the
`spring.cloud.dataflow.task.composedTaskRunnerName` property to the name
of your choice.  You can then register the composed task runner application with
the name you set using that property.

==== Configuring the Composed Task Runner application

The Composed Task Runner application has a `dataflow.server.uri` property that is used for validation and for launching child tasks. This defaults
to `http://localhost:9393`. If you run a distributed Spring Cloud Data Flow server, like you would do if you deploy the server on Cloud Foundry,
YARN or Kubernetes, then you need to provide the URI that can be used to access the server. You can either provide this `dataflow.server.uri`
property for the Composed Task Runner application when launching a composed task, or you can provide a `spring.cloud.dataflow.server.uri` property
for the Spring Cloud Data Flow server when it is started. For the latter case the `dataflow.server.uri` Composed Task Runner application property
will be automatically set when a composed task is launched.

=== Creating, Launching, and Destroying a Composed Task
==== Creating a Composed Task
The DSL for the composed tasks is used when creating a task definition via the
task create command. For example:
```
dataflow:> app register --name timestamp --type task --uri maven://org.springframework.cloud.task.app:timestamp-task:<DESIRED_VERSION>
dataflow:> app register --name mytaskapp --type task --uri file:///home/tasks/mytask.jar
dataflow:> task create my-composed-task --definition "mytaskapp && timestamp"
dataflow:> task launch my-composed-task
```
In the example above we assume that the applications to be used by our composed
task have not been registered yet.  So the first two steps we register two task
applications.  We then create our composed task definition by using the task
create command.  The composed task DSL in the example above will, when launched,
execute mytaskapp and then execute the timestamp application.

But before we launch the my-composed-task definition,  we can view what
Spring Cloud Data Flow generated for us.  This can be done by executing the
task list command.

```
dataflow:>task list
ââââââââââââââââââââââââââââ¤âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
â        Task Name         â                      Task Definition
â âââââââââââââââââââââââââââªâââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
âmy-composed-task          âmytaskapp && timestamp
âmy-composed-task-mytaskappâmytaskapp
âmy-composed-task-timestampâtimestamp
```
Spring Cloud Data Flow created three task definitions, one for each of the
applications that comprises our composed task (`my-composed-task-mytaskapp` and
`my-composed-task-timestamp`) as well as the composed task (`my-composed-task`)
definition.  We also see that each of the generated
names for the child tasks is comprised of the name of the composed task and
the name of the application separated by a dash `-`.  i.e. _my-composed-task_ `-`
_mytaskapp_.

===== Task Application Parameters
The task applications that comprise the composed task definition can also
contain parameters.  For example:
```
dataflow:> task create my-composed-task --definition "mytaskapp --displayMessage=hello && timestamp --format=YYYY"
```

==== Launching a Composed Task
Launching a composed task is done the same way as launching a stand-alone task.
i.e.
```
task launch my-composed-task
```
Once the task is launched and assuming all the tasks complete successfully you will
see three task executions when executing a `task execution list`.  For example:
```
dataflow:>task execution list
ââââââââââââââââââââââââââââ¤ââââ¤âââââââââââââââââââââââââââââ¤âââââââââââââââââââââââââââââ¤ââââââââââ
â        Task Name         âID â         Start Time         â          End Time          âExit Codeâ
â âââââââââââââââââââââââââââªââââªâââââââââââââââââââââââââââââªâââââââââââââââââââââââââââââªââââââââââ£
âmy-composed-task-timestampâ713âWed Apr 12 16:43:07 EDT 2017âWed Apr 12 16:43:07 EDT 2017â0        â
âmy-composed-task-mytaskappâ712âWed Apr 12 16:42:57 EDT 2017âWed Apr 12 16:42:57 EDT 2017â0        â
âmy-composed-task          â711âWed Apr 12 16:42:55 EDT 2017âWed Apr 12 16:43:15 EDT 2017â0        â
ââââââââââââââââââââââââââââ§ââââ§âââââââââââââââââââââââââââââ§âââââââââââââââââââââââââââââ§ââââââââââ
```
In the example above we see that my-compose-task launched and it also launched
the other tasks in sequential order and all of them executed successfully with
"Exit Code" as `0`.

===== Exit Statuses

The following list shows how the Exit Status will be set for each step (task)
contained in the composed task following each step execution.

* If the `TaskExecution` has an `ExitMessage` that will be used as the `ExitStatus`
* If no `ExitMessage` is present and the `ExitCode` is set to zero then the `ExitStatus`
for the step will be `COMPLETED`.
* If no `ExitMessage` is present and the `ExitCode` is set to any non zero number
then the `ExitStatus` for the step will be `FAILED`.

==== Destroying a Composed Task
The same command used to destroy a stand-alone task is the same as destroying a
composed task.  The only difference is that destroying a composed task will
also destroy the child tasks associated with it.   For example

```
dataflow:>task list
ââââââââââââââââââââââââââââ¤âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
â        Task Name         â                      Task Definition
â âââââââââââââââââââââââââââªâââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
âmy-composed-task          âmytaskapp && timestamp
âmy-composed-task-mytaskappâmytaskapp
âmy-composed-task-timestampâtimestamp

...
dataflow:>task destroy my-composed-task
dataflow:>task list
ââââââââââââââââââââââââââââ¤âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
â        Task Name         â                      Task Definition
â âââââââââââââââââââââââââââªâââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
ââââââââââââââââââââââââââââ§âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
```
==== Stopping a Composed Task
In cases where a composed task execution needs to be stopped.  This can be done
via the:

* RESTful API
* Spring Cloud Data Flow Dashboard by selecting the Job's tab and then
clicking the stop button by the job execution that needs to be stopped.

The composed task run will be stopped
when the currently running child task completes.  The step associated with the
child task that was running at the time that the composed task was stopped will
be marked as `STOPPED` as well as the composed task job execution.

==== Restarting a Composed Task
In cases where a composed task fails during execution and the status of the
composed task is `FAILED` then the task can be restarted.  This can be done
via the:

* RESTful API
* Shell by launching the task using the same parameters
* Spring Cloud Data Flow Dashboard by selecting the Job's tab and then
clicking the restart button by the job execution that needs to be restarted.

NOTE: Restarting a Composed Task job that has been stopped (via the
Spring Cloud Data Flow Dashboard or RESTful API), will relaunch  the
`STOPPED` child task, and then launch the remaining (unlaunched) child tasks
in the specified order.

=== Composed Task DSL

==== Conditional Execution
Conditional execution is expressed using a double ampersand symbol `&&`.
This allows each task in the sequence to be launched only if the previous task
successfully completed. For example:
```
task create my-composed-task --definition "foo && bar"
```

When the composed task my-composed-task is launched, it will launch the
task `foo` and if it completes successfully, then the task `bar` will be
launched. If the `foo` task fails, then the task `bar` will not launch.

You can also use the Spring Cloud Data Flow Dashboard to create your conditional
execution. By using the designer to drag and drop applications
that are required, and connecting them together to create your directed graph.
For example:

.Conditional Execution
image::{dataflow-asciidoc}/images/dataflow-ctr-conditional-execution.png[Composed Task Conditional Execution, scaledwidth="50%"]

The diagram above is a screen capture of the directed graph as it being created
using the Spring Cloud Data Flow Dashboard.  We see that are 4 components
in the diagram that comprise a conditional execution:

* Start icon - All directed graphs start from this symbol.  There will
only be one.
* Task icon - Represents each task in the directed graph.
* End icon - Represents the termination of a directed graph.
* Solid line arrow - Represents the flow conditional execution flow
between:
** Two applications
** The start control node and an application
** An application and the end control node

NOTE:  You can view a diagram of your directed graph by clicking the detail
button next to the composed task definition on the definitions tab.

==== Transitional Execution
The DSL supports fine grained control over the transitions taken during the
execution of the directed graph. Transitions are specified by providing a
condition for equality based on the exit status of the previous task.
A task transition is represented by the following symbol `-&gt;`.

===== Basic Transition
A basic transition would look like the following:

```
task create my-transition-composed-task --definition "foo 'FAILED' -> bar 'COMPLETED' -> baz"
```

In the example above `foo` would launch and if it had an exit status of `FAILED`,
then the `bar` task would launch. If the exit status of `foo` was `COMPLETED`
then `baz` would launch. All other statuses returned by `foo` will have no effect
and task would terminate normally.

Using the Spring Cloud Data Flow Dashboard to create  the same "basic
transition" would look like:

.Basic Transition
image::{dataflow-asciidoc}/images/dataflow-ctr-transition-basic.png[Composed Task Basic Transition, scaledwidth="50%"]

The diagram above is a screen capture of the directed graph as it being created
using the Spring Cloud Data Flow Dashboard.  Notice that there are 2 different
types of connectors:

* Dashed line - Is the line used to represent transitions from the application
to one of the possible destination applications.
* Solid line - Used to connect applications in a conditional execution or a
connection between the application and a control node (end, start).

When creating a transition, link the application to each of possible
destination using the connector.  Once complete go to each connection and
select it by clicking it.  A bolt icon should appear, click that icon and
enter the exit status required for that connector.  The solid line for that
connector will turn to a dashed line.

===== Transition With a Wildcard
Wildcards are supported for transitions by the DSL for example:
```
task create my-transition-composed-task --definition "foo 'FAILED' -> bar '*' -> baz"
```

In the example above `foo` would launch and if it had an exit status of `FAILED`,
then the `bar` task would launch. Any exit status of `foo` other than `FAILED`
then `baz` would launch.

Using the Spring Cloud Data Flow Dashboard to create the same
"transition with wildcard" would look like:

.Basic Transition With Wildcard
image::{dataflow-asciidoc}/images/dataflow-ctr-transition-basic-wildcard.png[Composed Task Basic Transition with Wildcard, scaledwidth="50%"]

===== Transition With a Following Conditional Execution
A transition can be followed by a conditional execution so long as the wildcard
is not used. For example:
```
task create my-transition-conditional-execution-task --definition "foo 'FAILED' -> bar 'UNKNOWN' -> baz && qux && quux"
```

In the example above `foo` would launch and if it had an exit status of `FAILED`,
then the `bar` task would launch.  If `foo` had an exit status of `UNKNOWN` then
`baz` would launch.  Any exit status of `foo` other than `FAILED` or `UNKNOWN`
then `qux` would launch and upon successful completion `quux` would launch.

Using the Spring Cloud Data Flow Dashboard to create the same
"transition with conditional execution" would look like:

.Transition With Conditional Execution
image::{dataflow-asciidoc}/images/dataflow-ctr-transition-conditional-execution.png[Composed Task Transition with Conditional Execution, scaledwidth="50%"]

NOTE:  In this diagram we see the dashed line (transition) connecting the `foo` application
to the target applications, but a solid line connecting the conditional executions
between `foo`, `qux`, and  `quux`.

==== Split Execution
Splits allow for multiple tasks within a composed task to be run in parallel.
It is denoted by using angle brackets <> to group tasks and flows that are to
be run in parallel. These tasks and flows are separated by the double pipe `||`
. For example:
```
task create my-split-task --definition "<foo || bar || baz>"
```
The example above will launch tasks `foo`, `bar` and `baz` in parallel.

Using the Spring Cloud Data Flow Dashboard to create the same
"split execution" would look like:

.Split
image::{dataflow-asciidoc}/images/dataflow-ctr-split.png[Composed Task Split, scaledwidth="50%"]

With the task DSL a user may also execute multiple split groups
in succession. For example:
```
task create my-split-task --definition "<foo || bar || baz> && <qux || quux>"
```

In the example above tasks `foo`, `bar` and `baz` will be launched in parallel,
once they all complete then tasks `qux`, `quux` will be launched in parallel.
Once they complete the composed task will end.   However if `foo`, `bar`, or
`baz` fails then, the split containing `qux` and `quux` will not launch.

Using the Spring Cloud Data Flow Dashboard to create the same
"split with multiple groups" would look like:

.Split as a part of a conditional execution
image::{dataflow-asciidoc}/images/dataflow-ctr-multiple-splits.png[Composed Task Split, scaledwidth="50%"]

Notice that there is a `SYNC` control node that is by the designer when
connecting two consecutive splits.

===== Split Containing Conditional Execution
A split can also have a conditional execution within the angle brackets.  For
example:
```
task create my-split-task --definition "<foo && bar || baz>"
```
In the example above we see that `foo` and `baz` will be launched in parallel,
however `bar` will not launch until `foo` completes successfully.

Using the Spring Cloud Data Flow Dashboard to create the same
"split containing conditional execution" would look like:

.Split with conditional execution
image::{dataflow-asciidoc}/images/dataflow-ctr-split-contains-conditional.png[Composed Task Split With Conditional Execution, scaledwidth="50%"]


